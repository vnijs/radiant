<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />



<title>Regression &gt; Linear (OLS)</title>

<script src="libs/jquery-1.11.0/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.1/css/united.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.1/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.1/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.1/shim/respond.min.js"></script>

<style type="text/css">

/* padding for bootstrap navbar */
body {
  padding-top: 50px;
  padding-bottom: 40px;
}

/* offset scroll position for anchor links (for fixed navbar)  */
.section h2 {
  padding-top: 55px;
  margin-top: -55px;
}
.section h3 {
  padding-top: 55px;
  margin-top: -55px;
}

/* don't use link color in navbar */
.dropdown-menu>li>a {
  color: black;
}

</style>

<link rel="stylesheet" href="libs/font-awesome-4.1.0/css/font-awesome.min.css"/>
<link rel="shortcut icon" type="image/png" href="/radiant/images/radiant_red.png"/>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="libs/highlight/textmate.css"
      type="text/css" />
<script src="libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img { 
  max-width:100%; 
  height: auto; 
}
</style>
<div class="container-fluid main-container">


<div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://github.com/mostly-harmless/radiant">Radiant</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li><a href="/radiant/index.html">Home</a></li>
        <li class="dropdown">
          <a href="/radiant/base/" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">Data<span class="caret"></span></a>
          <ul class="dropdown-menu" role="menu">
             <li><a href="/radiant/base/manage.html">Manage</a></li>
             <li><a href="/radiant/base/view.html">View</a></li>
             <li><a href="/radiant/base/visualize.html">Visualize</a></li>
             <li><a href="/radiant/base/explore.html">Explore</a></li>
             <li><a href="/radiant/base/pivot.html">Pivot</a></li>
             <li><a href="/radiant/base/merge.html">Merge</a></li>
             <li><a href="/radiant/base/transform.html">Transform</a></li>
          </ul>
        </li>

        <li class="dropdown">
          <a href="/radiant/quant/" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">Quant<span class="caret"></span></a>
          <ul class="dropdown-menu" role="menu">
             <li class="dropdown-header">Random</li>
             <li><a href="/radiant/quant/ctl.html">Central Limit Theorem</a></li>
             <li><a href="/radiant/quant/sampling.html">Sampling</a></li>
             <li><a href="/radiant/quant/sample_size.html">Sample size</a></li>
             <li class="divider"></li>
             <li class="dropdown-header">Base</li>
             <li><a href="/radiant/quant/single_mean.html">Single mean</a></li>
             <li><a href="/radiant/quant/compare_means.html">Compare means</a></li>
             <li><a href="/radiant/quant/single_prop.html">Single proportion</a></li>
             <li><a href="/radiant/quant/compare_props.html">Compare proportions</a></li>
             <li><a href="/radiant/quant/cross_tabs.html">Cross-tabs</a></li>
             <li class="divider"></li>
             <li class="dropdown-header">Regression</li>
             <li><a href="/radiant/quant/correlation.html">Correlation</a></li>
             <li><a href="/radiant/quant/regression.html">Linear (OLS)</a></li>
             <li><a href="/radiant/quant/glm_reg.html">GLM</a></li>
          </ul>
        </li>

        <li class="dropdown">
          <a href="/radiant/marketing/" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">Marketing<span class="caret"></span></a>
          <ul class="dropdown-menu" role="menu">
             <li class="dropdown-header">Random</li>
             <li><a href="/radiant/quant/ctl.html">Central Limit Theorem</a></li>
             <li><a href="/radiant/quant/sampling.html">Sampling</a></li>
             <li><a href="/radiant/quant/sample_size.html">Sample size</a></li>
             <li class="divider"></li>
             <li class="dropdown-header">Base</li>
             <li><a href="/radiant/quant/single_mean.html">Single mean</a></li>
             <li><a href="/radiant/quant/compare_means.html">Compare means</a></li>
             <li><a href="/radiant/quant/single_prop.html">Single proportion</a></li>
             <li><a href="/radiant/quant/compare_props.html">Compare proportion</a></li>
             <li><a href="/radiant/quant/cross_tabs.html">Cross-tabs</a></li>
             <li class="divider"></li>
             <li class="dropdown-header">Regression</li>
             <li><a href="/radiant/quant/correlation.html">Correlation</a></li>
             <li><a href="/radiant/quant/regression.html">Linear (OLS)</a></li>
             <li><a href="/radiant/quant/glm_reg.html">GLM</a></li>
             <li class="divider"></li>
             <li class="dropdown-header">Maps</li>
             <li><a href="/radiant/marketing/mds.html">(Dis)similarity</a></li>
             <li><a href="/radiant/marketing/percepmap.html">Attributes</a></li>
             <li class="divider"></li>
             <li class="dropdown-header">Factor</li>
             <li><a href="/radiant/marketing/pre_factor.html">Pre-factor</a></li>
             <li><a href="/radiant/marketing/full_factor.html">Factor</a></li>
             <li class="divider"></li>
             <li class="dropdown-header">Cluster</li>
             <li><a href="/radiant/marketing/hier_clus.html">Hierarchical</a></li>
             <li><a href="/radiant/marketing/kmeans_clus.html">K-means</a></li>
             <li class="divider"></li>
             <li class="dropdown-header">Conjoint</li>
             <li><a href="/radiant/marketing/conjoint_profiles.html">Create profiles</a></li>
             <li><a href="/radiant/marketing/conjoint.html">Conjoint</a></li>
          </ul>
        </li>
        <li><a href="/radiant/programming.html">Programming</a></li>
        <li><a href="/radiant/about.html">About</a></li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">
<h1 class="title">Regression &gt; Linear (OLS)</h1>
</div>


<hr />
<blockquote>
<p>(Linear) Regression: The workhorse of empirical research in the social sciences</p>
</blockquote>
<p>All example files discussed below can be loaded from the Data &gt; Manage page. Click the <code>examples</code> radio button and press <code>Load examples</code>.</p>
<div id="functionality" class="section level4">
<h4>Functionality</h4>
<p>Start by selecting a dependent variable and one or more independents variables. If two or more Independent variables are included in the model we may want to investigate if any interactions are present. An interaction exists when the effect of an independent variable on the dependent variable is determined, at least partially, by the level of another independent variable. For example, the increase in price for a 1 versus a 2 carrot diamond may depend on the clarity level of the diamond.</p>
<p>The <code>Predict</code> box allows you calculate predicted values from a regression model. You must specify at least one variable and value to get a prediction. If you do not specify a value for each variable in the model either the mean value or the most frequent factor level will be used. It is only possible to predict outcomes based on variables in the model (e.g., <code>carat</code> must one of the selected independent variables to predict the <code>price</code> of a 2-carat diamond)</p>
<ul>
<li>To predict the price of a 1-carat diamond type <code>carat = 1</code> and press enter</li>
<li>To predict the price of diamonds ranging from .5 to 1 carat at steps of size .05 type <code>carat = seq(.5.1,.05)</code> and press enter</li>
<li>To predict the price of 1,2, or 3 carat diamonds with an ideal cut type <code>carat = 1:3, cut = &quot;Ideal&quot;</code> and press enter</li>
</ul>
<p>We can test if two or more variables together add significantly to the fit of a model. This function can be very useful to test if the overall influence of a variable of type <code>factor</code> is significant.</p>
<p>Various additional outputs and options can be selected:</p>
<ul>
<li>RMSE: Root Mean Squared Error (Prediction error)</li>
<li>Sum of Squares: The total variance in the dependent variable split into the variance explained by the model and the remainder</li>
<li>VIF: Variance Inflation Factors and Rsq. These are measures of multi-collinearity for the independent variables</li>
<li>Standardized coefficients: Coefficients may be hard to compare of the independent variables are measured on different scales. By standardizing the data before estimation we can see which variables move-the-needle most</li>
<li>Step-wise: A data-mining approach to select the best fitting model</li>
</ul>
</div>
<div id="example-1-catalog-sales" class="section level4">
<h4>Example 1: Catalog sales</h4>
<p>We have access to data from a company selling men’s and women’s apparel through mail-order catalogs (dataset <code>catalog</code>). The company maintains a database on past and current customers’ value and characteristics. Value is determined as the total $ sales to the customer in the last year. The data are a random sample of 200 customers from the company’s database. The r-data contains a data frame with 200 observations on 4 variables</p>
<ul>
<li>Sales = Total sales (in $) to a household in the past year</li>
<li>Income = Household income ($1000)</li>
<li>HH.size = Size of the household (# of people)</li>
<li>Age = Age of the head of the household</li>
</ul>
<p>The catalog company is interested in redesigning their Customer Relationship Management (CRM) strategy. We will proceed in two steps:</p>
<ol style="list-style-type: decimal">
<li>Estimate a regression model using last year’s sales total. Dependent variable: sales total for each of the 200 households; explanatory variables: household income (measured in thousands of dollars), size of household, and age of the household head. The data-set is given in the <code>catalog</code> data file. Interpret each of your estimated coefficients. Also provide a statistical evaluation of the model as a whole.</li>
<li>Which explanatory variables are significant predictors of customer value (use a 95% confidence level)?</li>
</ol>
<p><strong>Answer:</strong></p>
<p>Output from Radiant (Regression &gt; Linear (OLS)) is provided below:</p>
<p><img src="figures_quant/regression_ex1a.png" alt="Regression 1 - summary" /></p>
<p>The F-statistic suggests that the regression model as a whole explains a significant amount of variance in Sales. The calculated F-value is equal to 32.33 and has a very small p-value (&lt; 0.001). The amount of variance in sales explained by the model is equal to 33.1%</p>
<p>The null and alternate hypothesis for the F-test test can be formulated as follows: H0: All regression coefficients are equal to 0 Ha: At least one regression coefficient is not equal to zero</p>
<p>The coefficients from the regression can be interpreted as follows:</p>
<ul>
<li>For an increase in income of $1000 we expect, on average, to see an increase in sales of $1.7754, keeping all other variables constant.</li>
<li>For an increase in household size of 1 person we expect, on average, to see an increase in sales of $22.1218, keeping all other variables constant.</li>
<li>For an increase in the age of the head of the household of 1 year we expect, on average, to see an increase in sales of $0.45, keeping all other variables constant.</li>
</ul>
<p>For each of the independent variables the following null and alternate hypotheses can be formulated: H0: The coefficient associated with independent variable X is equal to 0 Ha: The coefficient associated with independent variable X is not equal to 0</p>
<p>The coefficients for <code>Income</code> and <code>HH.size</code> are both significant (p-values &lt; 0.05), i.e., we can reject H0 for each of these coefficients. The coefficient for Age HH is not significant (p-value &gt; 0.05), i.e., we cannot reject H0 for Age HH. We conclude that a change in Age of the household head does not lead to a significant change in sales.</p>
</div>
<div id="example-2-ideal-data-for-regression" class="section level4">
<h4>Example 2: Ideal data for regression</h4>
<p>The data <code>ideal</code> contains simulated data that is very useful to demonstrate what data for and residuals from a regression should ideally look like. The r-data file contains a data-frame with 1000 observations on 4 variables. y is the dependent variable and x1, x2, and x3 are independent variables. The plots shown below can be used as a bench mark for regressions on real world data. We will use Regression &gt; Linear (OLS) to conduct the analysis. First go the the Plots tab and select y as the dependent variable and x1, x2, and x3 as the independent variables.</p>
<p>y, x2, and x3 appear roughly normally distributed whereas x1 appears roughly uniformly distributed. No indication of outliers or severely skewed distributions.</p>
<p><img src="figures_quant/regression_ideal_hist.png" alt="Regression 2 - ideal histograms" /></p>
<p>In the plot of correlations there are clear associations among the dependent and independent variables as well as among the independent variables themselves. Recall that in an experiment the x’s of interest would have a zero correlation. The scatter plots in the lower-diagonal part of the plot show that the relationships between the variables are (approximately) linear.</p>
<p><img src="figures_quant/regression_ideal_corr.png" alt="Regression 2 - ideal correlations" /></p>
<p>The scatter plots of y (the dependent variable) against each of the independent variables confirm the insight from the correlation plot. The line fitted through the scatter plots is sufficiently flexible that it would pickup any non-linearities. The lines are, however, very straight suggesting that a basic linear will likely be appropriate.</p>
<p><img src="figures_quant/regression_ideal_scatter.png" alt="Regression 2 - ideal scatter" /></p>
<p>The dashboard of six residual plots looks excellent, as we might expect for these data. True values and predicted values from the regression form a straight line with random scatter, i.e., as the actual values of the dependent variable go up, so do the predicted values from the model. The residuals (i.e., the differences between the values of the dependent variable data and the values predicted by the regression) show no pattern and are randomly scattered around a horizontal axis. Any pattern would suggest that the model is better (or worse) at predicting some parts of the data compared to others. If a pattern were visible in the Residual vs Row order plot we might be concerned about auto-correlation. Again, the residuals are nicely scattered about a horizontal axis. Note that auto-correlation is problem we are concerned about when we have time-series data. The Q-Q plot shows a nice straight and diagonal line, evidence that the residuals are normally distributed. This conclusion is confirmed by the histogram of the residuals and the density plot of the residuals (green) versus the theoretical density of a normally distributed variable (blue line).</p>
<p><img src="figures_quant/regression_ideal_dashboard.png" alt="Regression 2 - ideal dashboard" /></p>
<p>The final diagnostic we will discuss is a set of plots of the residuals versus the independent variables (or predictors). There is no indication of any trends or heteroscedasticity. Any patterns in these plots would be cause for concern. There are also no outliers, i.e., points that are far from the main cloud of data points.</p>
<p><img src="figures_quant/regression_ideal_res_vs_pred.png" alt="Regression 2 - ideal residual vs predicted" /></p>
<p>Since the diagnostics look good, we can draw inferences from the regression. First, the model is significant as a whole: the p-value on the F-statistic is less than 0.05 therefore we reject the null hypothesis that all three variables in the regression have slope equal to zero. Second, each variable is statistically significant: for example, the p-value on the t-statistic for x1 is less than 0.05 therefore we reject the null hypothesis that x1 has slope equal to zero when x2 and x3 are also in the model (i.e., ‘holding all other variables constant’).</p>
<p>Increases in x1 and x3 are associated with increases in y whereas increases in x2 are associated with decreases in y. Since these are simulated data the exact interpretation of the coefficient is not interesting. However, in the scatterplot it looks like increases in x3 are associated with decreases in y. What explains the difference? Hint: consider the correlation plots.</p>
<p><img src="figures_quant/regression_ideal_summary.png" alt="Regression 2 - ideal summary" /></p>
</div>
<div id="example-3-linear-or-log-log-regression" class="section level4">
<h4>Example 3: Linear or log-log regression?</h4>
<p>In marketing both linear and log-log regressions are very common. In this example we will look for evidence in the data and residuals that may which model specification is more appropriate for the available data.</p>
<p>The data <code>diamonds</code> contains information on prices of 3000 diamonds. A more complete description of the data and variables is available from the Data &gt; Manage page. Select the variable <code>price</code> as the dependent variable and <code>carat</code> and <code>clarity</code> as the independent variables. Before looking at the parameter estimates from the regression go to the Plots tab to take a look at the data and residuals. Below are the set of histograms for the variables in the model. Prices and carats seem skewed to the right. Note that the direction of skew is determined by where the <em>tail</em> is.</p>
<p><img src="figures_quant/regression_diamonds_hist.png" alt="Regression 3 - histograms" /></p>
<p>In the plot of correlations there are clear associations among the dependent and independent variables. The correlation between price and carat is extremely large (i.e., .93). The correlation between carat and clarity of the diamond is significant and negative.</p>
<p><img src="figures_quant/regression_diamonds_corr.png" alt="Regression 3 - correlations" /></p>
<p>The scatter plots of price (the dependent variable) against the independent variables are not as clean as for the ‘ideal’ data in example 2. The line fitted through the scatter plots is sufficiently flexible to pickup non-linearities. The line for carat seems to have some curvature and the points do not look randomly scattered around that line. In fact the points seem to fan-out for higher prices and number of carats. There does not seem to be very much movement in price for different levels of clarity. If anything, the price of the diamond seems to go down as clarity increase. A surprising result we will discuss in more detail below.</p>
<p><img src="figures_quant/regression_diamonds_scatter.png" alt="Regression 3 - scatter" /></p>
<p>The dashboard of six residual plots looks less than stellar. The true values and predicted values from the regression form an S-shaped curve. At higher actual and predicted values the spread of points around the line is wider, consistent with what we saw in the scatter plot of price versus carats. The residuals (i.e., the differences between the actual data and the values predicted by the regression) show an even more distinct pattern as they are clearly not randomly scattered around a horizontal axis. The Residual vs Row order plot looks perfectly straight indicating that auto-correlation is not a concern. Finally, while for the <code>ideal</code> data in example 2 the Q-Q plot showed a nice straight diagonal line, here dots clearly separate from the line at the right extreme, evidence that the residuals are not normally distributed. This conclusions is confirmed by the histogram and density plots of the residuals that show a more spiked appearance than a truly normally distributed variable would.</p>
<p><img src="figures_quant/regression_diamonds_dashboard.png" alt="Regression 3 - dashboard" /></p>
<p>The final diagnostic we will discuss is a set of plots of the residuals versus the independent variables (or predictors). The residuals fan-out from left to right in the plot of residuals vs carats. The box-plot of clarity versus residuals shows outliers with strong negative values for lower levels of clarity and outliers with strong positive values for diamonds with higher levels of clarity. <img src="figures_quant/regression_diamonds_res_vs_pred.png" alt="Regression 3 - residual vs predicted" /></p>
<p>Since the diagnostics do not look good, we should <strong>not</strong> draw inferences from this regression. A log-log specification may be preferable.</p>
<p>We will apply a log transformation to both price and carat and rerun the analysis to see if the log-log specification is more appropriate for the data. This transformation can be done in Data &gt; Transform. Select the variables price and carat. Choose <code>change</code> from the Transformation type drop-down and choose <code>Log</code> from the Apply function drop-down. Make sure to <code>Save changes</code> so the new variables are added to the dataset. Note that we cannot apply a log transformation to clarity because it is a <a href="http://en.wikipedia.org/wiki/Categorical_variable" target="_blank">categorical</a> variable.</p>
<p>In Regression &gt; Linear (OLS) select the variable <code>log_price</code> as the dependent variable and <code>log_carat</code> and <code>clarity</code> as the independent variables. Before looking at the parameter estimates from the regression go to the Plots tab to take a look at the data and residuals. Below are the set of histograms for the variables in the model. log_price and log_carat are no longer right skewed, a good sign.</p>
<p><img src="figures_quant/regression_log_diamonds_hist.png" alt="Regression 3 - log histograms" /></p>
<p>In the plot of correlations there are still clear associations among the dependent and independent variables. The correlation between log_price and log_carat is extremely large (i.e., .93). The correlation between log_carat and clarity of the diamond is significant and negative.</p>
<p><img src="figures_quant/regression_log_diamonds_corr.png" alt="Regression 3 - log correlations" /></p>
<p>The scatter plots of log_price (the dependent variable) against the independent variables are now much cleaner. The line through the scatter plot of log_price versus log_carat is (mostly) straight. Although the points do have a bit of a blocked shape around the line the scattering seem mostly random. We no longer see the points fan-out for higher values of log_price and log_carat. There seems to be a bit more movement in log_price for different levels of clarity. However, the log_price of the diamond still goes down as clarity increase which is unexpected. We will discuss this result below.</p>
<p><img src="figures_quant/regression_log_diamonds_scatter.png" alt="Regression 3 - log scatter" /></p>
<p>The dashboard of six residual plots looks much better than for the linear model. The true values and predicted values from the regression (almost) form a straight line. At higher and lower actual and predicted values the line is perhaps still slightly curved. The residuals are much closer to a random scatter around a horizontal axis. The Residual vs Row order plot still looks perfectly straight indicating that auto-correlation is not a concern. Finally, the Q-Q plot shows a nice straight and diagonal line, just like we saw for the <code>ideal</code> data in example 2, Evidence that the residuals are now normally distributed. This conclusion is confirmed by the histogram and density plot of the residuals.</p>
<p><img src="figures_quant/regression_log_diamonds_dashboard.png" alt="Regression 3 - log dashboard" /></p>
<p>The final diagnostic we will discuss is a set of plots of the residuals versus the independent variables (or predictors). The residuals look much closer to random scatter around a horizontal line compared to the linear model, although for low (high) values of log_carat the residuals may be a bit higher (lower). The box-plot of clarity versus residuals now only shows a few outliers.</p>
<p><img src="figures_quant/regression_log_diamonds_res_vs_pred.png" alt="Regression 3 - log residual vs predicted" /></p>
<p>Since the diagnostics now look much better, we can feel more confident about drawing inferences from this regression. The regression results are available in the Summary tab. Note that we get 7 coefficients for the variable clarity compared to only one for <code>log_carat</code>. How come? If you look at the data description (Data &gt; Manage) you will see that clarity is a categorical variables with levels that go from IF (worst clarity) to I1 (best clarity). Categorical variables must be converted to a set of dummy (or indicator) variables before we can apply numerical analysis tools like regression. Each dummy indicates if a particular diamond has a particular clarity level (=1) or not (=0). Interestingly, to capture all information in the 8-level clarity variable we only need 7 dummy variables. Note there is no dummy variable for the clarity level I1 because we don’t actually need it in the regression. When a diamond is <strong>not</strong> of clarity SI2, SI1, VS2, VS1, VVS2, VVS1 or IF we know that in our data it must therefore be of clarity I1.</p>
<p>The F-statistic suggests that the regression model as a whole explains a significant amount of variance in log_price. The calculated F-value is very large and has a very small p-value (&lt; 0.001) so we can reject the null hypothesis that all regression coefficients are equal to zero. The amount of variance in log_price explained by the model is equal to 96.6. It seems likely that prices of diamonds are much easier to predict than demand for diamonds.</p>
<p>The null and alternate hypothesis for the F-test test can be formulated as follows: H0: All regression coefficients are equal to 0 Ha: At least one regression coefficient is not equal to zero</p>
<p>The coefficients from the regression can be interpreted as follows:</p>
<ul>
<li>For a 1% increase in carats we expect, on average, to see a 1.809% increase in the price of a diamond of, keeping all other variables constant</li>
<li>Compared to a diamond of clarity I1 we expect, on average, to pay 100x(exp(.444)-1) = 55.89% more for a diamond of clarity SI2, keeping all other variables constant</li>
<li>Compared to a diamond of clarity I1 we expect, on average, to pay 100x(exp(.591)-1) = 80.58% more for a diamond of clarity SI1, keeping all other variables constant</li>
<li>Compared to a diamond of clarity I1 we expect, on average, to pay 100x(exp(1.080)-1) = 194.47% more for a diamond of clarity IF, keeping all other variables constant</li>
</ul>
<p>The coefficients for each of the levels of clarity imply that an increase in clarity will increase the price of diamond. Why then did the boxplot of clarity versus (log) price show price decreasing with clarity? The difference is that in a regression we can determine the effect of a change in one variable (e.g., clarity) keeping all else constant (e.g., carat). Bigger, heavier, diamonds are more likely to have flaws compared to small diamonds so when we look at the boxplot we are really seeing the effect of not only improving clarity on price but also the effect of carats which are negatively correlated with clarity. In a regression we can compare the effects of different levels of clarity on (log) price for a diamond of <strong>the same size</strong> (i.e., keeping carat constant). Without (log) carat in the model the estimated effect of clarity would be incorrect due to <a href="http://en.wikipedia.org/wiki/Omitted-variable_bias" target="_blank">omitted variable bias</a>. In fact, from a regression of log_price on clarity we would conclude that a diamond of the highest clarity in the data (IF) would cost 59.22% less compared to a diamond of the lowest clarity (I1). Clearly this is not a sensible conclusion.</p>
<p>For each of the independent variables the following null and alternate hypotheses can be formulated: H0: The coefficient associated with independent variable X is equal to 0 Ha: The coefficient associated with independent variable X is not equal to 0</p>
<p>All coefficients in this regression are highly significant.</p>
<p><img src="figures_quant/regression_log_diamonds_summary.png" alt="Regression 3 - log summary" /></p>
</div>
<div id="technical-notes" class="section level4">
<h4>Technical notes</h4>
<div id="coefficient-interpretation-for-linear-model" class="section level5">
<h5>Coefficient interpretation for linear model</h5>
<p>To illustrate the interpretation of coefficients in a regression model we start with the following equation:</p>
<p><br /><span class="math"><em>S</em><sub><em>t</em></sub> = <em>a</em> + <em>b</em><em>P</em><sub><em>t</em></sub> + <em>c</em><em>D</em><sub><em>t</em></sub> + <em>ϵ</em><sub><em>t</em></sub></span><br /></p>
<p>where <span class="math"><em>S</em><sub><em>t</em></sub></span> is sales in units at time <span class="math"><em>t</em></span>, <span class="math"><em>P</em><sub><em>t</em></sub></span> is the price in $ at time <span class="math"><em>t</em></span>, <span class="math"><em>D</em><sub><em>t</em></sub></span> is a dummy variable that indicates if a product is on display in a given week, and <span class="math"><em>ϵ</em><sub><em>t</em></sub></span> is the error term.</p>
<p>For a continuous variable such as price we can determine the effect of a $1 change, while keeping all other variables constant, by taking the partial derivative of the sales equation with respect to <span class="math"><em>P</em></span>.</p>
<p><br /><span class="math">$$
  \frac{ \partial S_t }{ \partial P_t } = b
$$</span><br /></p>
<p>So <span class="math"><em>b</em></span> is the marginal effect on sales of a $1 change in price. Because a dummy variable such as <span class="math"><em>D</em></span> is not continuous we cannot use differentiation and the approach needed to determine the marginal effect is a little different. If we compare sales levels when <span class="math"><em>D</em> = 1</span> to sales levels when <span class="math"><em>D</em> = 0</span> we see that</p>
<p><br /><span class="math"><em>a</em> + <em>b</em><em>P</em><sub><em>t</em></sub> + <em>c</em> × 1 − <em>a</em> + <em>b</em><em>P</em><sub><em>t</em></sub> + <em>c</em> × 0 = <em>c</em></span><br /></p>
<p>For a linear model <span class="math"><em>c</em></span> is the marginal effect on sales when the product is on display.</p>
</div>
<div id="coefficient-interpretation-for-a-semi-log-model" class="section level5">
<h5>Coefficient interpretation for a semi-log model</h5>
<p>To illustrate the interpretation of coefficients in a semi-log regression model we start with the following equation:</p>
<p><br /><span class="math"><em>l</em><em>n</em><em>S</em><sub><em>t</em></sub> = <em>a</em> + <em>b</em><em>P</em><sub><em>t</em></sub> + <em>c</em><em>D</em><sub><em>t</em></sub> + <em>ϵ</em><sub><em>t</em></sub></span><br /></p>
<p>where <span class="math"><em>l</em><em>n</em><em>S</em><sub><em>t</em></sub></span> is the (natural) log of sales at time <span class="math"><em>t</em></span>. For a continuous variable such as price we can again determine the effect of a $1 change, while keeping all other variables constant, by taking the partial derivative of the sales equation with respect to <span class="math"><em>P</em></span>. For the left-hand side of the equation we can use the chain-rule to get</p>
<p><br /><span class="math">$$
  \frac {\partial ln S_t}{\partial P_t} = \frac{1}{S_t} \frac{\partial S_t}{\partial P_t}
$$</span><br /></p>
<p>In words, the derivative of the natural logarithm of a variable is the reciprocal of that variable, times the derivative of that variable. From the discussion on the linear model above we know that <span class="math">$\frac{ \partial a + b P_t + c D_t}{ \partial P_t } = b$</span>. Combining these two equations gives</p>
<p><br /><span class="math">$$
  \frac {1}{S_t} \frac{\partial S_t}{\partial P_t} = b \; \text{or} \; \frac {\Delta S_t}{S_t} \approx b
$$</span><br /></p>
<p>So a $1 change in price leads to a <span class="math"><em>b</em></span>% change in sales. Because a dummy variable such as <span class="math"><em>D</em></span> is not continuous we cannot use differentiation and will again compare sales levels when <span class="math"><em>D</em> = 1</span> to sales levels when <span class="math"><em>D</em> = 0</span> to get <span class="math">$\frac {\Delta S_t}{S_t}$</span>. To get <span class="math"><em>S</em><sub><em>t</em></sub></span> rather than <span class="math"><em>l</em><em>n</em><em>S</em><sub><em>t</em></sub></span> on the left hand side we take the exponent of both sides. This gives <span class="math"><em>S</em><sub><em>t</em></sub> = <em>e</em><sup><em>a</em> + <em>b</em><em>P</em><sub><em>t</em></sub> + <em>c</em><em>D</em><sub><em>t</em></sub></sup></span>. The percentage change in <span class="math"><em>S</em><sub><em>t</em></sub></span> when <span class="math"><em>D</em><sub><em>t</em></sub></span> changes from 0 to 1 is then given by:</p>
<p><br /><span class="math">$$
  \begin{aligned}
  \frac {\Delta S_t}{S_t} &amp;\approx \frac{ e^{a + b P_t + c\times 1} - e^{a + b P_t + c \times 0} } {e^{a + b P_t + c \times 0} }\\
  &amp;= \frac{ e^{a + b P_t} e^c - e^{a + b P_t} }{ e^{a + b P_t} }\\
  &amp;= e^c - 1
  \end{aligned}
$$</span><br /></p>
<p>For the semi-log model 100 <span class="math"> ×  <em>c</em></span> is the percentage change in sales when the product is on display.</p>
</div>
<div id="coefficient-interpretation-for-a-log-log-model" class="section level5">
<h5>Coefficient interpretation for a log-log model</h5>
<p>To illustrate the interpretation of coefficients in a log-log regression model we start with the following equation:</p>
<p><br /><span class="math"><em>l</em><em>n</em><em>S</em><sub><em>t</em></sub> = <em>a</em> + <em>b</em><em>l</em><em>n</em><em>P</em><sub><em>t</em></sub> + <em>ϵ</em><sub><em>t</em></sub></span><br /></p>
<p>where <span class="math"><em>l</em><em>n</em><em>P</em><sub><em>t</em></sub></span> is the (natural) log of sales at time <span class="math"><em>t</em></span>. Ignoring the error term for simplicity we can rewrite this model in its multiplicative form by taking the exponent on both sides:</p>
<p><br /><span class="math">$$
  \begin{aligned}
  S_t &amp;= e^a + e^{b ln P_t}\\
  S_t &amp;= a^* P^b_t
  \end{aligned}
$$</span><br /></p>
<p>For a continuous variable such as price we can again take the partial derivative of the sales equation with respect to <span class="math"><em>P</em><sub><em>t</em></sub></span> to get the marginal effect.</p>
<p><br /><span class="math">$$
  \begin{aligned}
  \frac{\partial S_t}{\partial P_t} &amp;= b a^* P^{b-1}_t\\
  &amp;= b S_t P^{-1}_t\\
  &amp;= b \frac{S_t}{P_t}
  \end{aligned}
$$</span><br /></p>
<p>The general formula for an elasticity is <span class="math">$\frac{\partial S_t}{\partial P_t} \frac{P_t}{S_t}$</span>. Adding this information to the equation above we see that the coefficient <span class="math"><em>b</em></span> estimated from a log-log regression can be directly interpreted as an elasticity:</p>
<p><br /><span class="math">$$
  \frac{\partial S_t}{\partial P_t} \frac{P_t}{S_t} = b \frac{S_t}{P_t} \frac{P_t}{S_t} = b
$$</span><br /></p>
<p>So a 1% change in price leads to a <span class="math"><em>b</em></span>% change in sales.</p>
</div>
</div>


&copy; Vincent Nijs (2015) <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank"><img alt="Creative Commons License" style="border-width:0" src="/radiant/images/80x15.png" /></a>

<!-- some extra javascript for older browsers -->
<script type="text/javascript" src="libs/polyfill.js"></script>

<!-- mathjax -->
<script type='text/javascript' src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

<script>

// manage active state of menu based on current page
$(document).ready(function () {

    // active menu
    href = window.location.pathname
    href = href.substr(href.lastIndexOf('/') + 1)
    $('a[href="' + href + '"]').parent().addClass('active');

    // manage active menu header
    if (href.startsWith('authoring_'))
      $('a[href="' + 'authoring' + '"]').parent().addClass('active');
    else if (href.endsWith('_format.html'))
      $('a[href="' + 'formats' + '"]').parent().addClass('active');
    else if (href.startsWith('developer_'))
      $('a[href="' + 'developer' + '"]').parent().addClass('active');

});

</script>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>


</body>
</html>
